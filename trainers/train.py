import pandas as pd
from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import numpy as np
import pickle as pkl
import time

def debug(input: any, filename: str = "debug.txt"):
    with open(f"debug/{filename}", "a+") as file:
        # if the file has data already, clear it
        file.seek(0)
        file.write(input)

print("[*] Reading data...")
start_time = time.time()

malwareData = pd.read_csv('data/MalwareData.csv', sep='|')

legitimateData = malwareData[0:41323].drop(["legitimate"], axis=1)
malwaresData = malwareData[41323::].drop(["legitimate"], axis=1)

pd.set_option("display.max_columns", None)

data_in = malwareData.drop(["Name", "md5", "legitimate"], axis=1).values
labels = malwareData["legitimate"].values

print("	=> Data read: %d samples with %d features" % (data_in.shape[0], data_in.shape[1]))

debug(f"Data Read: Labels: \n {malwaresData.head}\n")

end_time = time.time()
print(" => Data reading completed in %f seconds" % (end_time - start_time))

print("[*] Training the model...")
start_time = time.time()

extratrees = ExtraTreesClassifier().fit(data_in, labels)
select = SelectFromModel(extratrees, prefit=True)

data_in_new = select.transform(data_in)

features = data_in_new.shape[1]
importances = extratrees.feature_importances_
indices = np.argsort(importances)[::-1]

for f in range(features):
    debug(f"{f+1}. feature {malwareData.columns[2+indices[f]]} ({importances[indices[f]]})\n", "features.txt")

legit_trainer, legit_test, mal_trainer, mal_test = train_test_split(data_in_new, labels, test_size=0.2)

legit_trainer = pd.DataFrame(legit_trainer, columns=[malwareData.columns[2+indices[f]] for f in range(features)])
legit_test = pd.DataFrame(legit_test, columns=[malwareData.columns[2+indices[f]] for f in range(features)])

classifier = RandomForestClassifier(n_estimators=50)
classifier.fit(legit_trainer, mal_trainer)

result = classifier.predict(legit_test)

print("	=> Training completed in %f seconds" % (time.time() - start_time))

## Calculate the false positive rate

print("[*] Calculating the false positive rate...")
start_time = time.time()

confusion_matrix_product = confusion_matrix(mal_test, result)
confusion_matrix_result = ((confusion_matrix_product[0][1] / float(sum(confusion_matrix_product[0])))*100)
print("	=> RandomForest: False positive rate : %f %%" % confusion_matrix_result)

gradient_boosting = GradientBoostingClassifier(n_estimators=50)
gradient_boosting.fit(legit_trainer, mal_trainer)

result = gradient_boosting.predict(legit_test)
confusion_matrix_product2 = confusion_matrix(mal_test, result)
confusion_matrix_result2 = ((confusion_matrix_product2[0][1] / float(sum(confusion_matrix_product2[0])))*100)
print("	=> Gradient: False positive rate : %f %%" % confusion_matrix_result2)

end_time = time.time()
print("	=> False positive rate calculation completed in %f seconds" % (end_time - start_time))


## Compare the results of the two models
if (confusion_matrix_result < confusion_matrix_result2):
    print("[!] RandomForest is better; saving the model")
    
    with open('data/trained_model.pkl', 'wb') as file:
        pkl.dump(classifier, file)
else:
    print("[!] Gradient is better; saving the model")
    
    with open('data/trained_model.pkl', 'wb') as file:
        pkl.dump(gradient_boosting, file)
